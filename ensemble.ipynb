{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# import necessary packages"
      ],
      "metadata": {
        "id": "IB80SGB_mqVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zidi7knuk6XG",
        "outputId": "6aa75d1a-88d9-4b2c-cd40-3973657f0a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyyaml h5py\n",
        "!pip install transformers\n",
        "!pip install scipy\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import TFRobertaModel\n",
        "import tensorflow as tf\n",
        "from transformers import RobertaTokenizer, RobertaModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import data & cleaning"
      ],
      "metadata": {
        "id": "7Gfqb-JLmvy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmwP2UKsk_Sd",
        "outputId": "be5f85c8-f7bd-4438-f83e-8ddf1eb82d58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "worksheet = gc.open('PennsylvaniaMidterms11Aug11Oct2').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "import pandas as pd\n",
        "df=pd.DataFrame.from_records(rows)\n",
        "\n",
        "new_header = df.iloc[0] #grab the first row for the header\n",
        "df = df[1:] #take the data less the header row\n",
        "df.columns = new_header #set the header row as the df header\n",
        "df['SentimentScore']= pd.to_numeric(df['SentimentScore'])\n",
        "df['Subjectivity']= pd.to_numeric(df['Subjectivity'])\n",
        "df['Sarcasm']= pd.to_numeric(df['Sarcasm'])\n",
        "df['Faction']= pd.to_numeric(df['Faction'])\n",
        "final_df=df.loc[df['SentimentScore'].isin([0,1,2])]"
      ],
      "metadata": {
        "id": "MdYmrcDDlBPt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "def clean_content(content):\n",
        "  content=re.sub('#','',content)\n",
        "  content = re.sub(r'@[^(JohnFetterman)][^(DrOz)]\\w+', '', content)\n",
        "  content = re.sub('@', '', content)\n",
        "  content = re.sub(r'\\\\u\\w+', '', content)\n",
        "  content = re.sub(r\"http\\S+\", \"\", content)\n",
        "  content = re.sub(\"\\n\", \"\", content)\n",
        "  # content = re.sub(r'[^\\w\\s\\.]', '', content)\n",
        "  # content=' '.join(s for s in content.split() if not any(c.isdigit() for c in s))\n",
        "  # content=content.lower()\n",
        "  return content\n",
        "\n",
        "final_df['cleaned_tweets']=final_df['Content'].apply(clean_content)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def nltk2wn_tag(nltk_tag):\n",
        "  if nltk_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif nltk_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif nltk_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif nltk_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:                    \n",
        "    return None\n",
        "def lemmatize_sentence(content,lemmatizer=lemmatizer):\n",
        "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(content))    \n",
        "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
        "  res_words = []\n",
        "  for word, tag in wn_tagged:\n",
        "    if tag is None:             \n",
        "      res_words.append(word.lower())\n",
        "    else:\n",
        "      res_words.append(lemmatizer.lemmatize(word.lower(), tag))\n",
        "  content=\" \".join(res_words)\n",
        "  content = re.sub(r'[^\\w\\s]', '', content)\n",
        "  content=' '.join(s for s in content.split() if not any(c.isdigit() for c in s))\n",
        "  return content\n",
        "final_df['lemmatized_tweets']=final_df['cleaned_tweets'].apply(lemmatize_sentence)\n",
        "sentiment=final_df[final_df['SentimentScore']<2]\n",
        "sentiment=sentiment[sentiment['Faction']<2]\n",
        "sentiment=sentiment.dropna(subset=['Faction'])\n",
        "support=sentiment['SentimentScore'].values\n",
        "faction=sentiment['Faction'].values\n",
        "raw_sent=[]\n",
        "for i in range(len(faction)):\n",
        "  if faction[i]!=support[i]:\n",
        "    if support[i]==0:\n",
        "      raw_sent.append(1)\n",
        "    else:\n",
        "      raw_sent.append(0)\n",
        "  else:\n",
        "    raw_sent.append(support[i])\n",
        "sentiment['RawSentiment']=raw_sent\n",
        "sentiment=sentiment[['cleaned_tweets','lemmatized_tweets','RawSentiment']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvmFBzFflI8l",
        "outputId": "e2d47e7e-3e87-4268-e4c3-ecb114da63d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "47Za00iXmdqG",
        "outputId": "18bc1c7b-6aa8-49e8-d714-d8b884745efa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                        cleaned_tweets  \\\n",
              "1     VOTE FOR JohnFetterman .Republican DrOz  will ...   \n",
              "2     the_vello Why does he always look like a SLOB?...   \n",
              "3     John Fetterman is a POS! Just a trust found ki...   \n",
              "4     DrOz I would love to see a sit-down, face-to-f...   \n",
              "5     Baphomet is non binary and babies were slain f...   \n",
              "...                                                 ...   \n",
              "1147  JohnFetterman Democrats destroy everything the...   \n",
              "1148  John Fetterman wipes Black Lives Matter sectio...   \n",
              "1158  ProudElephantUS That useless bum is no good an...   \n",
              "1391  DrOz JohnFetterman  RacistFetterman JohnFetterman   \n",
              "1548  He's a real winner, isn't he? JohnFetterman is...   \n",
              "\n",
              "0                                     lemmatized_tweets  RawSentiment  \n",
              "1     vote for johnfetterman republican droz will no...           1.0  \n",
              "2     the_vello why do he always look like a slob he...           1.0  \n",
              "3     john fetterman be a po just a trust find kid w...           1.0  \n",
              "4     droz i would love to see a sitdown facetoface ...           1.0  \n",
              "5     baphomet be non binary and baby be slay for mo...           1.0  \n",
              "...                                                 ...           ...  \n",
              "1147  johnfetterman democrat destroy everything they...           1.0  \n",
              "1148  john fetterman wipe black life matter section ...           1.0  \n",
              "1158  proudelephantus that useless bum be no good an...           1.0  \n",
              "1391   droz johnfetterman racistfetterman johnfetterman           1.0  \n",
              "1548  he s a real winner be nt he johnfetterman be b...           1.0  \n",
              "\n",
              "[916 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-957fed79-8f36-416d-bdba-816555538fa4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleaned_tweets</th>\n",
              "      <th>lemmatized_tweets</th>\n",
              "      <th>RawSentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VOTE FOR JohnFetterman .Republican DrOz  will ...</td>\n",
              "      <td>vote for johnfetterman republican droz will no...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the_vello Why does he always look like a SLOB?...</td>\n",
              "      <td>the_vello why do he always look like a slob he...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>John Fetterman is a POS! Just a trust found ki...</td>\n",
              "      <td>john fetterman be a po just a trust find kid w...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DrOz I would love to see a sit-down, face-to-f...</td>\n",
              "      <td>droz i would love to see a sitdown facetoface ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Baphomet is non binary and babies were slain f...</td>\n",
              "      <td>baphomet be non binary and baby be slay for mo...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1147</th>\n",
              "      <td>JohnFetterman Democrats destroy everything the...</td>\n",
              "      <td>johnfetterman democrat destroy everything they...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1148</th>\n",
              "      <td>John Fetterman wipes Black Lives Matter sectio...</td>\n",
              "      <td>john fetterman wipe black life matter section ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1158</th>\n",
              "      <td>ProudElephantUS That useless bum is no good an...</td>\n",
              "      <td>proudelephantus that useless bum be no good an...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1391</th>\n",
              "      <td>DrOz JohnFetterman  RacistFetterman JohnFetterman</td>\n",
              "      <td>droz johnfetterman racistfetterman johnfetterman</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>He's a real winner, isn't he? JohnFetterman is...</td>\n",
              "      <td>he s a real winner be nt he johnfetterman be b...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>916 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-957fed79-8f36-416d-bdba-816555538fa4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-957fed79-8f36-416d-bdba-816555538fa4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-957fed79-8f36-416d-bdba-816555538fa4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# set seed"
      ],
      "metadata": {
        "id": "-TI1KQTamygs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "import os\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "import random \n",
        "random.seed(SEED)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "Wxa6-DrvlGel"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load base model"
      ],
      "metadata": {
        "id": "0zy6zl-Tm0Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load complex model\n",
        "tokenizer_roberta = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "def tokenize_roberta(data,max_len=128,tokenizer_roberta=tokenizer_roberta) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer_roberta.encode_plus(\n",
        "            data[i],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)\n",
        "\n",
        "def roberta_model(bert_model, max_len=128):\n",
        "    \n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    output = bert_model([input_ids,attention_masks])\n",
        "    output = output[1]\n",
        "    output = tf.keras.layers.Dense(128, activation='relu')(output)\n",
        "    output = tf.keras.layers.Dense(1)(output)\n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
        "    model.compile(opt, loss=loss, metrics=accuracy)\n",
        "    return model\n",
        "\n",
        "roberta = TFRobertaModel.from_pretrained('roberta-base')\n",
        "r_model = roberta_model(roberta)\n",
        "r_model.load_weights('/content/drive/MyDrive/model/sentiment.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HdBtmzZnpP-",
        "outputId": "64996dfb-e50b-4f97-facd-5eb2fdd0851e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define accuracy function"
      ],
      "metadata": {
        "id": "fpyn10BGohbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "def get_result(target,pred):\n",
        "  '''returns accuracy, f1, precision, recall'''\n",
        "  return accuracy_score(target,pred),f1_score(target,pred,average='weighted'),precision_score(target,pred),recall_score(target,pred)"
      ],
      "metadata": {
        "id": "sNqfCmtJokXV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output score"
      ],
      "metadata": {
        "id": "aYpr4g2NoRtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(min_df = 5,\n",
        "                             max_df = 0.8,\n",
        "                             sublinear_tf = True,\n",
        "                             use_idf = True)"
      ],
      "metadata": {
        "id": "BEcHg0b2piEP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {}\n",
        "    self.num_words = 0\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2index:\n",
        "      # First entry of word into vocabulary\n",
        "      self.word2index[word] = self.num_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.num_words] = word\n",
        "      self.num_words += 1\n",
        "    else:\n",
        "      # Word exists; increase word count\n",
        "      self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "hdnqz9P0piwm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "vocab=Vocabulary('tweet')\n",
        "for i in sentiment['lemmatized_tweets']:\n",
        "  result=wordpunct_tokenize(i)\n",
        "  for j in result:\n",
        "    vocab.add_word(j)\n",
        "def vectorize(input,vocab=vocab):\n",
        "  master_list=[]\n",
        "  for i in input:\n",
        "    result=wordpunct_tokenize(i)\n",
        "    tweet=[0]*len(vocab.word2index)\n",
        "    for j in result:\n",
        "      for k in vocab.word2index:\n",
        "        if j==k:\n",
        "          tweet[vocab.word2index[j]]=1\n",
        "    master_list.append(tweet)\n",
        "  return master_list"
      ],
      "metadata": {
        "id": "QTC2CNN8qdt2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import  MultinomialNB\n",
        "skf=KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "accuracy=[]\n",
        "f1=[]\n",
        "precision=[]\n",
        "recall=[]\n",
        "for train_index, test_index in skf.split(sentiment['cleaned_tweets'].values,sentiment['RawSentiment'].values):\n",
        "  X_train, X_test = sentiment['lemmatized_tweets'].values[train_index], sentiment['lemmatized_tweets'].values[test_index]\n",
        "  X_train_l,X_test_l=sentiment['cleaned_tweets'].values[train_index], sentiment['cleaned_tweets'].values[test_index]\n",
        "  y_train, y_test = sentiment['RawSentiment'].values[train_index],sentiment['RawSentiment'].values[test_index]\n",
        "  #svm\n",
        "  svm_model=svm.SVC()\n",
        "  train_vectors = vectorizer.fit_transform(X_train)\n",
        "  test_vectors = vectorizer.transform(X_test)\n",
        "  s_train = svm_model.fit(train_vectors,y_train).predict(train_vectors)\n",
        "  s_pred=svm_model.predict(test_vectors)\n",
        "  #bayes\n",
        "  bayes= MultinomialNB()\n",
        "  train_vectors = vectorize(X_train_l)\n",
        "  test_vectors = vectorize(X_test_l)\n",
        "  b_train=bayes.fit(train_vectors,y_train).predict(train_vectors)\n",
        "  b_pred=bayes.predict(test_vectors)\n",
        "  #roberta\n",
        "  train_input_ids, train_attention_masks = tokenize_roberta(X_train, 128)\n",
        "  result_roberta = r_model.predict([train_input_ids,train_attention_masks])\n",
        "  result_roberta=tf.math.sigmoid(result_roberta)\n",
        "  result_roberta=tf.reshape(result_roberta,(result_roberta.shape[0],)).numpy()\n",
        "  r_train=[]\n",
        "  for i in result_roberta:\n",
        "    if i<=0.5:\n",
        "      r_train.append(0)\n",
        "\n",
        "    else:\n",
        "      r_train.append(1)\n",
        "  test_input_ids, test_attention_masks = tokenize_roberta(X_test, 128)\n",
        "  result_roberta = r_model.predict([test_input_ids,test_attention_masks])\n",
        "  result_roberta=tf.math.sigmoid(result_roberta)\n",
        "  result_roberta=tf.reshape(result_roberta,(result_roberta.shape[0],)).numpy()\n",
        "  r_pred=[]\n",
        "  for i in result_roberta:\n",
        "    if i<=0.5:\n",
        "      r_pred.append(0)\n",
        "\n",
        "    else:\n",
        "      r_pred.append(1)\n",
        "  one_hot_train=np.transpose(np.stack([s_train,b_train,r_train]))\n",
        "  one_hot_pred=np.transpose(np.stack([s_pred,b_pred,r_pred]))\n",
        "  meta_model=svm.SVC()\n",
        "  y_pred=meta_model.fit(one_hot_train,y_train).predict(one_hot_pred)\n",
        "  from sklearn import svm\n",
        "  a,b,c,d=get_result(y_test,y_pred)\n",
        "  accuracy.append(a)\n",
        "  f1.append(b)\n",
        "  precision.append(c)\n",
        "  recall.append(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dUv6kXgoXit",
        "outputId": "5e1224df-6e3f-4947-bc80-611bc82fcc6e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 308s 13s/step\n",
            "6/6 [==============================] - 75s 12s/step\n",
            "23/23 [==============================] - 308s 13s/step\n",
            "6/6 [==============================] - 74s 12s/step\n",
            "23/23 [==============================] - 300s 13s/step\n",
            "6/6 [==============================] - 74s 12s/step\n",
            "23/23 [==============================] - 300s 13s/step\n",
            "6/6 [==============================] - 79s 13s/step\n",
            "23/23 [==============================] - 294s 13s/step\n",
            "6/6 [==============================] - 74s 12s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'accuracy: {sum(accuracy)/len(accuracy)}, f1: {sum(f1)/len(f1)}, precision: {sum(precision)/len(precision)}, recall: {sum(recall)/len(recall)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs7WvTZ234jY",
        "outputId": "9f9021b0-38f6-4e64-f70b-4f2f6a3f4b79"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8001544309812308, f1: 0.7890080096839339, precision: 0.8081734020001925, recall: 0.9271071496517813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "accuracy=[]\n",
        "f1=[]\n",
        "precision=[]\n",
        "recall=[]\n",
        "for train_index, test_index in skf.split(sentiment['cleaned_tweets'].values,sentiment['RawSentiment'].values):\n",
        "  X_train, X_test = sentiment['lemmatized_tweets'].values[train_index], sentiment['lemmatized_tweets'].values[test_index]\n",
        "  X_train_l,X_test_l=sentiment['cleaned_tweets'].values[train_index], sentiment['cleaned_tweets'].values[test_index]\n",
        "  y_train, y_test = sentiment['RawSentiment'].values[train_index],sentiment['RawSentiment'].values[test_index]\n",
        "  #svm\n",
        "  svm_model=svm.SVC()\n",
        "  train_vectors = vectorizer.fit_transform(X_train)\n",
        "  test_vectors = vectorizer.transform(X_test)\n",
        "  s_train = svm_model.fit(train_vectors,y_train).predict(train_vectors)\n",
        "  s_pred=svm_model.predict(test_vectors)\n",
        "  #bayes\n",
        "  bayes= MultinomialNB()\n",
        "  train_vectors = vectorize(X_train_l)\n",
        "  test_vectors = vectorize(X_test_l)\n",
        "  b_train=bayes.fit(train_vectors,y_train).predict(train_vectors)\n",
        "  b_pred=bayes.predict(test_vectors)\n",
        "  #roberta\n",
        "  train_input_ids, train_attention_masks = tokenize_roberta(X_train, 128)\n",
        "  result_roberta = r_model.predict([train_input_ids,train_attention_masks])\n",
        "  result_roberta=tf.math.sigmoid(result_roberta)\n",
        "  result_roberta=tf.reshape(result_roberta,(result_roberta.shape[0],)).numpy()\n",
        "  r_train=result_roberta\n",
        "  test_input_ids, test_attention_masks = tokenize_roberta(X_test, 128)\n",
        "  result_roberta = r_model.predict([test_input_ids,test_attention_masks])\n",
        "  result_roberta=tf.math.sigmoid(result_roberta)\n",
        "  result_roberta=tf.reshape(result_roberta,(result_roberta.shape[0],)).numpy()\n",
        "  r_pred=result_roberta\n",
        "  train=np.transpose(np.stack([s_train,b_train,r_train]))\n",
        "  pred=np.transpose(np.stack([s_pred,b_pred,r_pred]))\n",
        "  meta_model=LogisticRegression(random_state=SEED)\n",
        "  y_pred=meta_model.fit(train, y_train).predict(pred)\n",
        "  a,b,c,d=get_result(y_test,y_pred)\n",
        "  accuracy.append(a)\n",
        "  f1.append(b)\n",
        "  precision.append(c)\n",
        "  recall.append(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF2anB3w3-ND",
        "outputId": "d377a395-32d7-4da0-a841-9c28ab8197dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 299s 13s/step\n",
            "6/6 [==============================] - 79s 13s/step\n",
            "23/23 [==============================] - 295s 13s/step\n",
            "6/6 [==============================] - 80s 13s/step\n",
            "23/23 [==============================] - 293s 13s/step\n",
            "6/6 [==============================] - 79s 13s/step\n",
            "23/23 [==============================] - 294s 13s/step\n",
            "6/6 [==============================] - 79s 12s/step\n",
            "23/23 [==============================] - 297s 13s/step\n",
            "6/6 [==============================] - 75s 12s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'accuracy: {sum(accuracy)/len(accuracy)}, f1: {sum(f1)/len(f1)}, precision: {sum(precision)/len(precision)}, recall: {sum(recall)/len(recall)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTUExHpQCA2l",
        "outputId": "55925639-b4d3-49a4-91a4-5d30932361db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.805624851508672, f1: 0.7897912091049711, precision: 0.8002062515955609, recall: 0.9524818580292822\n"
          ]
        }
      ]
    }
  ]
}